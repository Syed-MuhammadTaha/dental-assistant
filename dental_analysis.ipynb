{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2+vteqY4i8U0TA5dlfpE5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Syed-MuhammadTaha/dental-assistant/blob/main/dental_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRxijQxY4CZn"
      },
      "outputs": [],
      "source": [
        "# Dental Radiograph Analysis Pipeline\n",
        "# End-to-End Implementation\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import efficientnet_v2_l\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 1. DATASET CLASSES\n",
        "#############################################\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, img_dir, json_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.json_dir = json_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get list of all images with json annotations\n",
        "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith('.png') or f.endswith('.jpg')]\n",
        "        self.img_files = [f for f in self.img_files if os.path.exists(os.path.join(json_dir, f.split('.')[0] + '.json'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Initialize mask and instance masks\n",
        "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.int32)\n",
        "        tooth_info = []\n",
        "\n",
        "        # Load annotations\n",
        "        json_path = os.path.join(self.json_dir, self.img_files[idx].split('.')[0] + '.json')\n",
        "        with open(json_path, 'r') as f:\n",
        "            ann_data = json.load(f)\n",
        "\n",
        "        # Process each tooth annotation (FDI notation)\n",
        "        for shape in ann_data['shapes']:\n",
        "            label = shape['label']  # FDI tooth number (e.g., \"21\")\n",
        "\n",
        "            # Skip if it's not a polygon or has no points\n",
        "            if shape['shape_type'] != 'polygon' or len(shape['points']) < 3:\n",
        "                continue\n",
        "\n",
        "            # Convert points to numpy array for polygon drawing\n",
        "            points = np.array(shape['points'], dtype=np.int32)\n",
        "\n",
        "            try:\n",
        "                # Parse FDI tooth number\n",
        "                tooth_id = int(label)\n",
        "\n",
        "                # Create binary mask for this tooth\n",
        "                tooth_mask = np.zeros_like(mask)\n",
        "                cv2.fillPoly(tooth_mask, [points], 1)\n",
        "\n",
        "                # Assign a unique ID for each tooth based on FDI number\n",
        "                mask[tooth_mask == 1] = tooth_id\n",
        "\n",
        "                # Store tooth info\n",
        "                tooth_info.append({\n",
        "                    'tooth_id': tooth_id,\n",
        "                    'points': points\n",
        "                })\n",
        "            except ValueError:\n",
        "                # Skip if label is not a valid number\n",
        "                print(f\"Skipping invalid tooth label: {label}\")\n",
        "                continue\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            # For image transforms only (mask should stay as integers)\n",
        "            transformed_image = self.transform(image)\n",
        "            sample = {\n",
        "                'image': transformed_image,\n",
        "                'mask': torch.from_numpy(mask).long(),\n",
        "                'tooth_info': tooth_info,\n",
        "                'img_path': img_path\n",
        "            }\n",
        "        else:\n",
        "            sample = {\n",
        "                'image': torch.from_numpy(image.transpose((2, 0, 1))).float() / 255.0,\n",
        "                'mask': torch.from_numpy(mask).long(),\n",
        "                'tooth_info': tooth_info,\n",
        "                'img_path': img_path\n",
        "            }\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "7sTALua04I2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, img_dir, json_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.json_dir = json_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all tooth subimages with condition annotations\n",
        "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith('.png') or f.endswith('.jpg')]\n",
        "        self.img_files = [f for f in self.img_files if os.path.exists(os.path.join(json_dir, f.split('.')[0] + '.json'))]\n",
        "\n",
        "        # Define tooth condition classes based on the dataset description\n",
        "        self.condition_classes = {\n",
        "            0: 'Tooth without anomalies',\n",
        "            1: 'Tooth with fillings',\n",
        "            2: 'Tooth with RCT',\n",
        "            3: 'Tooth with crown',\n",
        "            4: 'Tooth with caries',\n",
        "            5: 'Residual root',\n",
        "            6: 'Tooth with RCT and crown'\n",
        "        }\n",
        "\n",
        "        # For condition name to ID mapping\n",
        "        self.condition_name_to_id = {v: k for k, v in self.condition_classes.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Load annotations\n",
        "        json_path = os.path.join(self.json_dir, self.img_files[idx].split('.')[0] + '.json')\n",
        "        with open(json_path, 'r') as f:\n",
        "            ann_data = json.load(f)\n",
        "\n",
        "        # Get tooth condition label - should be in filename or annotation\n",
        "        # Try to extract from filename (if stored like \"21_3.png\" for tooth 21 with crown)\n",
        "        condition_label = 0  # Default: Tooth without anomalies\n",
        "\n",
        "        # Check for the condition in the group_id field (as requested)\n",
        "        if 'group_id' in ann_data and ann_data['group_id'] is not None:\n",
        "            try:\n",
        "                condition_label = int(ann_data['group_id'])\n",
        "            except (ValueError, TypeError):\n",
        "                pass  # If group_id exists but is not a valid integer\n",
        "        elif 'group_id' in ann_data:\n",
        "            # If group_id is null, it means no condition (default to 0)\n",
        "            condition_label = 0\n",
        "        # Check other places if group_id doesn't contain the condition\n",
        "        elif 'condition' in ann_data:\n",
        "            condition_label = int(ann_data['condition'])\n",
        "        elif 'attributes' in ann_data and 'condition' in ann_data['attributes']:\n",
        "            condition_label = int(ann_data['attributes']['condition'])\n",
        "        elif len(ann_data['shapes']) > 0 and 'attributes' in ann_data['shapes'][0]:\n",
        "            # Sometimes condition is stored in attributes of the shape\n",
        "            if 'condition' in ann_data['shapes'][0]['attributes']:\n",
        "                condition_label = int(ann_data['shapes'][0]['attributes']['condition'])\n",
        "        else:\n",
        "            # Try to extract from filename (format: \"toothID_conditionID.jpg\")\n",
        "            parts = os.path.splitext(self.img_files[idx])[0].split('_')\n",
        "            if len(parts) > 1 and parts[-1].isdigit():\n",
        "                condition_label = int(parts[-1])\n",
        "\n",
        "        # Make sure condition is in valid range (0-6)\n",
        "        condition_label = max(0, min(condition_label, 6))\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'condition': condition_label,\n",
        "            'condition_name': self.condition_classes[condition_label],\n",
        "            'img_path': img_path\n",
        "        }"
      ],
      "metadata": {
        "id": "osDdrHTG4Nme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 2. MODEL DEFINITIONS\n",
        "#############################################\n",
        "\n",
        "class YOLOv9SegmentationModel(nn.Module):\n",
        "    def __init__(self, num_classes=49):  # 48 teeth in FDI + supernumerary (91) + background (0)\n",
        "        super(YOLOv9SegmentationModel, self).__init__()\n",
        "        # For simplicity, we're implementing a U-Net style architecture\n",
        "        # In real implementation, load YOLOv9-e weights\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = self._make_layer(3, 64)\n",
        "        self.enc2 = self._make_layer(64, 128)\n",
        "        self.enc3 = self._make_layer(128, 256)\n",
        "        self.enc4 = self._make_layer(256, 512)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec4 = self._make_layer(512, 256)\n",
        "        self.dec3 = self._make_layer(512, 128)\n",
        "        self.dec2 = self._make_layer(256, 64)\n",
        "        self.dec1 = self._make_layer(128, 32)\n",
        "\n",
        "        # Output\n",
        "        self.final = nn.Conv2d(32, num_classes, kernel_size=1)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(nn.MaxPool2d(2)(e1))\n",
        "        e3 = self.enc3(nn.MaxPool2d(2)(e2))\n",
        "        e4 = self.enc4(nn.MaxPool2d(2)(e3))\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        d4 = self.dec4(e4)\n",
        "        d3 = self.dec3(torch.cat([nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)(d4), e3], dim=1))\n",
        "        d2 = self.dec2(torch.cat([nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)(d3), e2], dim=1))\n",
        "        d1 = self.dec1(torch.cat([nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)(d2), e1], dim=1))\n",
        "\n",
        "        out = self.final(d1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "TRBgmT4a4RcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientNetClassificationModel(nn.Module):\n",
        "    def __init__(self, num_classes=7):  # 7 dental conditions (0-6)\n",
        "        super(EfficientNetClassificationModel, self).__init__()\n",
        "\n",
        "        # Load pre-trained EfficientNetV2-L\n",
        "        self.efficientnet = efficientnet_v2_l(pretrained=True)\n",
        "\n",
        "        # Replace classifier\n",
        "        in_features = self.efficientnet.classifier[1].in_features\n",
        "        self.efficientnet.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.4, inplace=True),\n",
        "            nn.Linear(in_features=in_features, out_features=num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.efficientnet(x)"
      ],
      "metadata": {
        "id": "0EB3vef84UtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 3. TRAINING FUNCTIONS\n",
        "#############################################\n",
        "\n",
        "def train_segmentation_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10, scheduler=None):\n",
        "    model.train()\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_iou': []}\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "            images = batch['image'].to(device)\n",
        "            masks = batch['mask'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader)\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_iou = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
        "                images = batch['image'].to(device)\n",
        "                masks = batch['mask'].to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate IoU\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                iou = calculate_iou(preds, masks)\n",
        "                val_iou += iou\n",
        "\n",
        "        epoch_val_loss = val_loss / len(val_loader)\n",
        "        epoch_val_iou = val_iou / len(val_loader)\n",
        "\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['val_iou'].append(epoch_val_iou)\n",
        "\n",
        "        # Step scheduler if provided\n",
        "        if scheduler:\n",
        "            scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            torch.save(model.state_dict(), 'best_segmentation_model.pth')\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f}, Val IoU: {epoch_val_iou:.4f}\")\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "7ThT3FYE4Y7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(pred, target):\n",
        "    \"\"\"Calculate Intersection over Union for segmentation masks\"\"\"\n",
        "    # Convert to binary mask for each tooth\n",
        "    unique_classes = torch.unique(target)\n",
        "    total_iou = 0.0\n",
        "\n",
        "    # Skip background (0)\n",
        "    for cls in unique_classes:\n",
        "        if cls == 0:\n",
        "            continue\n",
        "\n",
        "        pred_mask = (pred == cls).float()\n",
        "        target_mask = (target == cls).float()\n",
        "\n",
        "        intersection = (pred_mask * target_mask).sum()\n",
        "        union = pred_mask.sum() + target_mask.sum() - intersection\n",
        "\n",
        "        if union > 0:\n",
        "            total_iou += (intersection / union).item()\n",
        "\n",
        "    # Return mean IoU across all teeth\n",
        "    if len(unique_classes) > 1:  # At least one tooth\n",
        "        return total_iou / (len(unique_classes) - 1)\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def train_classification_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10, scheduler=None):\n",
        "    model.train()\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['condition'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader)\n",
        "        epoch_train_acc = 100 * correct / total\n",
        "\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
        "                images = batch['image'].to(device)\n",
        "                labels = batch['condition'].to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = val_loss / len(val_loader)\n",
        "        epoch_val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        # Step scheduler if provided\n",
        "        if scheduler:\n",
        "            scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "            torch.save(model.state_dict(), 'best_classification_model.pth')\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, \"\n",
        "              f\"Train Acc: {epoch_train_acc:.2f}%, Val Loss: {epoch_val_loss:.4f}, \"\n",
        "              f\"Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "pH3IocQJ4cfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 4. INFERENCE/PREDICTION FUNCTIONS\n",
        "#############################################\n",
        "\n",
        "def process_tooth_region(tooth_image, classification_model, device, transform):\n",
        "    \"\"\"Process a single tooth region with the classification model\"\"\"\n",
        "    # Convert to PIL for transforms\n",
        "    tooth_pil = Image.fromarray(tooth_image)\n",
        "\n",
        "    # Apply transforms\n",
        "    tooth_tensor = transform(tooth_pil).unsqueeze(0).to(device)\n",
        "\n",
        "    # Get prediction\n",
        "    classification_model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = classification_model(tooth_tensor)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        conf, pred_class = torch.max(probs, 1)\n",
        "\n",
        "    return pred_class.item(), conf.item()\n",
        "\n",
        "\n",
        "def extract_tooth_regions(image, mask):\n",
        "    \"\"\"Extract individual tooth regions from the segmentation mask using FDI numbering\"\"\"\n",
        "    unique_ids = np.unique(mask)\n",
        "    tooth_regions = []\n",
        "\n",
        "    # Skip background (0)\n",
        "    for tooth_id in unique_ids:\n",
        "        if tooth_id == 0:\n",
        "            continue\n",
        "\n",
        "        # Create binary mask for this tooth\n",
        "        tooth_mask = (mask == tooth_id).astype(np.uint8)\n",
        "\n",
        "        # Find bounding box\n",
        "        contours, _ = cv2.findContours(tooth_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if contours:\n",
        "            x, y, w, h = cv2.boundingRect(contours[0])\n",
        "\n",
        "            # Add margin (ROI*1.5 as shown in diagram)\n",
        "            margin = int(max(w, h) * 0.25)  # 1.5x = original + 0.5x margin\n",
        "            x_min = max(0, x - margin)\n",
        "            y_min = max(0, y - margin)\n",
        "            x_max = min(image.shape[1], x + w + margin)\n",
        "            y_max = min(image.shape[0], y + h + margin)\n",
        "\n",
        "            # Extract region\n",
        "            tooth_region = image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            # For FDI numbering, we directly use the tooth_id\n",
        "            # In FDI notation, 11-48 are standard teeth, 91 is supernumerary\n",
        "            tooth_regions.append({\n",
        "                'id': int(tooth_id),  # FDI tooth number\n",
        "                'region': tooth_region,\n",
        "                'bbox': (x_min, y_min, x_max, y_max)\n",
        "            })\n",
        "\n",
        "    return tooth_regions\n",
        "\n",
        "\n",
        "def generate_results_visualization(image, mask, tooth_predictions):\n",
        "    \"\"\"Create visualization with colored segmentation and tooth numbering\"\"\"\n",
        "    # Create copy of the image\n",
        "    vis_image = image.copy()\n",
        "\n",
        "    # Define colors for visualization (colorful as in the image)\n",
        "    colors = [\n",
        "        (0, 255, 0),    # Green\n",
        "        (255, 0, 0),    # Red\n",
        "        (0, 0, 255),    # Blue\n",
        "        (255, 255, 0),  # Yellow\n",
        "        (255, 0, 255),  # Magenta\n",
        "        (0, 255, 255),  # Cyan\n",
        "        (128, 0, 0),    # Maroon\n",
        "        (0, 128, 0),    # Dark Green\n",
        "        (0, 0, 128),    # Navy\n",
        "        (128, 128, 0),  # Olive\n",
        "        (128, 0, 128),  # Purple\n",
        "    ]\n",
        "\n",
        "    # Create overlay mask\n",
        "    overlay = np.zeros_like(vis_image)\n",
        "    unique_ids = np.unique(mask)\n",
        "\n",
        "    # Create a mapping of tooth ID to consistent color\n",
        "    # This ensures each tooth type (incisors, canines, etc.) gets same color\n",
        "    tooth_color_map = {}\n",
        "    for tooth_id in unique_ids:\n",
        "        if tooth_id == 0:\n",
        "            continue  # Skip background\n",
        "\n",
        "        # FDI notation: first digit is quadrant, second is tooth position\n",
        "        # Use position for color to ensure symmetry\n",
        "        if tooth_id == 91:  # Supernumerary\n",
        "            tooth_position = 9\n",
        "        else:\n",
        "            tooth_position = tooth_id % 10\n",
        "\n",
        "        if tooth_position not in tooth_color_map:\n",
        "            tooth_color_map[tooth_position] = colors[tooth_position % len(colors)]\n",
        "\n",
        "    # Color each tooth\n",
        "    for tooth_id in unique_ids:\n",
        "        if tooth_id == 0:\n",
        "            continue  # Skip background\n",
        "\n",
        "        tooth_position = tooth_id % 10 if tooth_id != 91 else 9\n",
        "        color = tooth_color_map[tooth_position]\n",
        "        overlay[mask == tooth_id] = color\n",
        "\n",
        "    # Blend with original image\n",
        "    alpha = 0.5\n",
        "    vis_image = cv2.addWeighted(vis_image, 1-alpha, overlay, alpha, 0)\n",
        "\n",
        "    # Add tooth numbers and conditions\n",
        "    for pred in tooth_predictions:\n",
        "        tooth_id = pred['tooth_id']\n",
        "        condition = pred['condition_name']\n",
        "        bbox = pred['bbox']\n",
        "\n",
        "        # Get center point of tooth for labeling\n",
        "        center_x = (bbox[0] + bbox[2]) // 2\n",
        "        center_y = (bbox[1] + bbox[3]) // 2\n",
        "\n",
        "        # Draw tooth number\n",
        "        cv2.putText(vis_image, str(tooth_id), (center_x, center_y),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "        # Draw condition indicator (small circle with color based on condition)\n",
        "        condition_colors = {\n",
        "            'Tooth without anomalies': (0, 255, 0),      # Green\n",
        "            'Tooth with fillings': (0, 255, 255),        # Cyan\n",
        "            'Tooth with RCT': (0, 0, 255),               # Blue\n",
        "            'Tooth with crown': (255, 0, 255),           # Magenta\n",
        "            'Tooth with caries': (255, 0, 0),            # Red\n",
        "            'Residual root': (128, 0, 0),                # Maroon\n",
        "            'Tooth with RCT and crown': (255, 255, 0)    # Yellow\n",
        "        }\n",
        "\n",
        "        condition_color = condition_colors.get(condition, (255, 255, 255))\n",
        "        cv2.circle(vis_image, (center_x, center_y + 15), 5, condition_color, -1)\n",
        "\n",
        "    return vis_image\n",
        "\n",
        "\n",
        "def generate_dental_report(tooth_predictions):\n",
        "    \"\"\"Generate the dental report as shown in the image\"\"\"\n",
        "    # Define condition names according to the dataset\n",
        "    condition_names = {\n",
        "        0: 'Tooth without anomalies',\n",
        "        1: 'Tooth with fillings',\n",
        "        2: 'Tooth with RCT',\n",
        "        3: 'Tooth with crown',\n",
        "        4: 'Tooth with caries',\n",
        "        5: 'Residual root',\n",
        "        6: 'Tooth with RCT and crown'\n",
        "    }\n",
        "\n",
        "    # Count each condition\n",
        "    condition_counts = {name: 0 for name in condition_names.values()}\n",
        "\n",
        "    # Missing teeth detection (compare with expected FDI numbers)\n",
        "    # Standard full dentition has teeth 11-18, 21-28, 31-38, 41-48\n",
        "    all_teeth_fdi = set(list(range(11, 19)) + list(range(21, 29)) +\n",
        "                         list(range(31, 39)) + list(range(41, 49)))\n",
        "\n",
        "    # Get the teeth we've found\n",
        "    found_teeth = set(pred['tooth_id'] for pred in tooth_predictions)\n",
        "\n",
        "    # Missing teeth are those in all_teeth_fdi but not in found_teeth\n",
        "    missing_teeth = all_teeth_fdi - found_teeth\n",
        "\n",
        "    # Count conditions for found teeth\n",
        "    for pred in tooth_predictions:\n",
        "        condition_counts[pred['condition_name']] += 1\n",
        "\n",
        "    # Add missing teeth count\n",
        "    condition_counts['Missing tooth'] = len(missing_teeth)\n",
        "\n",
        "    # Format the report similar to the image\n",
        "    report = {\n",
        "        'Missing tooth': condition_counts['Missing tooth'],\n",
        "        'Tooth with fillings': condition_counts['Tooth with fillings'],\n",
        "        'Tooth with RCT': condition_counts['Tooth with RCT'],\n",
        "        'Tooth with crown': condition_counts['Tooth with crown'],\n",
        "        'Tooth with caries': condition_counts['Tooth with caries'],\n",
        "        'Residual root': condition_counts['Residual root'],\n",
        "        'Tooth with RCT + crown': condition_counts['Tooth with RCT and crown']\n",
        "    }\n",
        "\n",
        "    return report\n",
        "\n",
        "\n",
        "def predict_panoramic_radiograph(image_path, seg_model, cls_model, device, seg_transform, cls_transform):\n",
        "    \"\"\"End-to-end prediction pipeline for a panoramic dental radiograph\"\"\"\n",
        "    # Load image\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Prepare for segmentation model\n",
        "    input_tensor = seg_transform(Image.fromarray(image)).unsqueeze(0).to(device)\n",
        "\n",
        "    # Segment teeth\n",
        "    seg_model.eval()\n",
        "    with torch.no_grad():\n",
        "        seg_output = seg_model(input_tensor)\n",
        "        seg_mask = torch.argmax(seg_output, dim=1).cpu().numpy()[0]\n",
        "\n",
        "    # Extract individual tooth regions\n",
        "    tooth_regions = extract_tooth_regions(image, seg_mask)\n",
        "\n",
        "    # Initialize condition class mapping\n",
        "    condition_mapping = {\n",
        "        0: 'Tooth without anomalies',\n",
        "        1: 'Tooth with fillings',\n",
        "        2: 'Tooth with RCT',\n",
        "        3: 'Tooth with crown',\n",
        "        4: 'Tooth with caries',\n",
        "        5: 'Residual root',\n",
        "        6: 'Tooth with RCT and crown'\n",
        "    }\n",
        "\n",
        "    # Process each tooth with classification model\n",
        "    tooth_predictions = []\n",
        "    for tooth in tooth_regions:\n",
        "        condition_id, confidence = process_tooth_region(\n",
        "            tooth['region'], cls_model, device, cls_transform\n",
        "        )\n",
        "\n",
        "        tooth_predictions.append({\n",
        "            'tooth_id': tooth['id'],\n",
        "            'condition': condition_id,\n",
        "            'condition_name': condition_mapping[condition_id],\n",
        "            'confidence': confidence,\n",
        "            'bbox': tooth['bbox']\n",
        "        })\n",
        "\n",
        "    # Generate visualization\n",
        "    result_image = generate_results_visualization(image, seg_mask, tooth_predictions)\n",
        "\n",
        "    # Generate report\n",
        "    report = generate_dental_report(tooth_predictions)\n",
        "\n",
        "    return {\n",
        "        'original_image': image,\n",
        "        'segmentation_mask': seg_mask,\n",
        "        'result_visualization': result_image,\n",
        "        'tooth_predictions': tooth_predictions,\n",
        "        'report': report\n",
        "    }"
      ],
      "metadata": {
        "id": "Nm9IObbe4hpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_teeth_from_panoramics(dataset_info, seg_transform=None, device=None):\n",
        "    \"\"\"Extract individual teeth from panoramic radiographs for classification training\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    img_dir = dataset_info['img_dir']\n",
        "    json_dir = dataset_info['json_dir']\n",
        "    teeth_output_dir = dataset_info['teeth_output_dir']\n",
        "    teeth_ann_dir = dataset_info['teeth_ann_dir']\n",
        "    paired_files = dataset_info['paired_files']\n",
        "\n",
        "    # Load segmentation model for extracting teeth\n",
        "    seg_model = YOLOv9SegmentationModel(num_classes=49)  # 48 teeth + background\n",
        "\n",
        "    # Try to load pre-trained weights if available\n",
        "    if os.path.exists('best_segmentation_model.pth'):\n",
        "        seg_model.load_state_dict(torch.load('best_segmentation_model.pth', map_location=device))\n",
        "        print(\"Loaded pre-trained segmentation model.\")\n",
        "\n",
        "    seg_model = seg_model.to(device)\n",
        "    seg_model.eval()\n",
        "\n",
        "    # Default transform if none provided\n",
        "    if seg_transform is None:\n",
        "        seg_transform = transforms.Compose([\n",
        "            transforms.Resize((512, 512)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    # Process each panoramic image\n",
        "    extracted_count = 0\n",
        "\n",
        "    for img_file in tqdm(paired_files, desc=\"Extracting teeth\"):\n",
        "        # Load image\n",
        "        img_path = os.path.join(img_dir, img_file)\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            print(f\"Warning: Could not read image {img_path}\")\n",
        "            continue\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Load annotation\n",
        "        json_path = os.path.join(json_dir, os.path.splitext(img_file)[0] + '.json')\n",
        "        with open(json_path, 'r') as f:\n",
        "            ann_data = json.load(f)\n",
        "\n",
        "        # Process shapes to get tooth annotations\n",
        "        teeth_annotations = {}\n",
        "\n",
        "        for shape in ann_data['shapes']:\n",
        "            if shape['shape_type'] != 'polygon' or len(shape['points']) < 3:\n",
        "                continue\n",
        "\n",
        "            # Get tooth ID from label\n",
        "            try:\n",
        "                tooth_id = int(shape['label'])\n",
        "                points = np.array(shape['points'], dtype=np.int32)\n",
        "\n",
        "                # Get tooth condition from attributes or group_id\n",
        "                condition = 0  # Default: No anomalies\n",
        "\n",
        "                # Check for condition in group_id first\n",
        "                if 'group_id' in ann_data and ann_data['group_id'] is not None:\n",
        "                    try:\n",
        "                        condition = int(ann_data['group_id'])\n",
        "                    except (ValueError, TypeError):\n",
        "                        pass\n",
        "\n",
        "                # Next, check for condition in shape attributes\n",
        "                if ('group_id' not in ann_data or ann_data['group_id'] is None) and 'attributes' in shape:\n",
        "                    if 'condition' in shape['attributes']:\n",
        "                        try:\n",
        "                            condition = int(shape['attributes']['condition'])\n",
        "                        except (ValueError, TypeError):\n",
        "                            pass\n",
        "\n",
        "                # Check if condition is valid (0-6)\n",
        "                condition = max(0, min(condition, 6))\n",
        "\n",
        "                # Create binary mask for this tooth\n",
        "                tooth_mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
        "                cv2.fillPoly(tooth_mask, [points], 1)\n",
        "\n",
        "                # Find bounding box\n",
        "                contours, _ = cv2.findContours(tooth_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                if contours:\n",
        "                    x, y, w, h = cv2.boundingRect(contours[0])\n",
        "\n",
        "                    # Add margin (1.5x original size)\n",
        "                    margin = int(max(w, h) * 0.25)\n",
        "                    x_min = max(0, x - margin)\n",
        "                    y_min = max(0, y - margin)\n",
        "                    x_max = min(image.shape[1], x + w + margin)\n",
        "                    y_max = min(image.shape[0], y + h + margin)\n",
        "\n",
        "                    # Extract region\n",
        "                    tooth_region = image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "                    # Save tooth image with FDI_condition format\n",
        "                    tooth_img_name = f\"{tooth_id}_{condition}.png\"\n",
        "                    tooth_img_path = os.path.join(teeth_output_dir, tooth_img_name)\n",
        "                    cv2.imwrite(tooth_img_path, cv2.cvtColor(tooth_region, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                    # Create and save annotation for this tooth\n",
        "                    tooth_ann = {\n",
        "                        \"version\": \"1.0\",\n",
        "                        \"flags\": {},\n",
        "                        \"shapes\": [],\n",
        "                        \"imagePath\": tooth_img_name,\n",
        "                        \"imageHeight\": tooth_region.shape[0],\n",
        "                        \"imageWidth\": tooth_region.shape[1],\n",
        "                        \"group_id\": condition  # Store condition in group_id as requested\n",
        "                    }\n",
        "\n",
        "                    # Save annotation\n",
        "                    tooth_ann_path = os.path.join(teeth_ann_dir, os.path.splitext(tooth_img_name)[0] + '.json')\n",
        "                    with open(tooth_ann_path, 'w') as f:\n",
        "                        json.dump(tooth_ann, f, indent=2)\n",
        "\n",
        "                    extracted_count += 1\n",
        "\n",
        "            except ValueError:\n",
        "                # Skip if label is not a valid number\n",
        "                print(f\"Skipping invalid tooth label: {shape['label']}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"Extracted {extracted_count} individual teeth for classification training.\")\n",
        "    return extracted_count\n",
        "\n",
        "\n",
        "def dental_radiograph_analysis_pipeline(\n",
        "    data_dir,\n",
        "    output_dir,\n",
        "    batch_size=8,\n",
        "    num_epochs_seg=30,\n",
        "    num_epochs_cls=30,\n",
        "    learning_rate=0.001,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"Complete pipeline for dental radiograph analysis\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Prepare dataset\n",
        "    dataset_info = prepare_dataset(data_dir, output_dir)\n",
        "\n",
        "    # Define transforms\n",
        "    seg_transform = transforms.Compose([\n",
        "        transforms.Resize((512, 512)),  # Resize for consistency\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    cls_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create datasets and extract individual teeth if needed\n",
        "    # Check if we already have individual teeth extracted\n",
        "    teeth_files = [f for f in os.listdir(dataset_info['teeth_output_dir'])\n",
        "                   if f.endswith('.png') or f.endswith('.jpg')]\n",
        "\n",
        "    if len(teeth_files) < 100:  # Arbitrary threshold - extract teeth if we don't have enough\n",
        "        print(\"Extracting individual teeth from panoramic radiographs...\")\n",
        "        extract_teeth_from_panoramics(dataset_info, seg_transform, device)\n",
        "\n",
        "    # 1. SEGMENTATION MODEL TRAINING\n",
        "\n",
        "    # Create segmentation dataset\n",
        "    seg_dataset = SegmentationDataset(\n",
        "        img_dir=dataset_info['img_dir'],\n",
        "        json_dir=dataset_info['json_dir'],\n",
        "        transform=seg_transform\n",
        "    )\n",
        "\n",
        "    # Split dataset\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        range(len(seg_dataset)), test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        seg_dataset, batch_size=batch_size, sampler=train_sampler,\n",
        "        num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        seg_dataset, batch_size=batch_size, sampler=val_sampler,\n",
        "        num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize segmentation model\n",
        "    seg_model = YOLOv9SegmentationModel(num_classes=49).to(device)  # 48 teeth + background\n",
        "\n",
        "    # Check if we should load pretrained weights\n",
        "    if os.path.exists('best_segmentation_model.pth'):\n",
        "        seg_model.load_state_dict(torch.load('best_segmentation_model.pth', map_location=device))\n",
        "        print(\"Loaded pretrained segmentation model weights.\")\n",
        "    else:\n",
        "        # Define loss function and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(seg_model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Training segmentation model...\")\n",
        "        seg_history = train_segmentation_model(\n",
        "            seg_model, train_loader, val_loader, criterion, optimizer,\n",
        "            device, num_epochs=num_epochs_seg, scheduler=scheduler\n",
        "        )\n",
        "\n",
        "        # Plot training history\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(seg_history['train_loss'], label='Train Loss')\n",
        "        plt.plot(seg_history['val_loss'], label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(seg_history['val_iou'], label='Validation IoU')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('IoU')\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(output_dir, 'seg_training_history.png'))\n",
        "\n",
        "    # 2. CLASSIFICATION MODEL TRAINING\n",
        "\n",
        "    # Create classification dataset\n",
        "    cls_dataset = ClassificationDataset(\n",
        "        img_dir=dataset_info['teeth_output_dir'],\n",
        "        json_dir=dataset_info['teeth_ann_dir'],\n",
        "        transform=cls_transform\n",
        "    )\n",
        "\n",
        "    # Split dataset\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        range(len(cls_dataset)), test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        cls_dataset, batch_size=batch_size, sampler=train_sampler,\n",
        "        num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        cls_dataset, batch_size=batch_size, sampler=val_sampler,\n",
        "        num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize classification model\n",
        "    cls_model = EfficientNetClassificationModel(num_classes=7).to(device)  # 7 dental conditions\n",
        "\n",
        "    # Check if we should load pretrained weights\n",
        "    if os.path.exists('best_classification_model.pth'):\n",
        "        cls_model.load_state_dict(torch.load('best_classification_model.pth', map_location=device))\n",
        "        print(\"Loaded pretrained classification model weights.\")\n",
        "    else:\n",
        "        # Define loss function and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(cls_model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Training classification model...\")\n",
        "        cls_history = train_classification_model(\n",
        "            cls_model, train_loader, val_loader, criterion, optimizer,\n",
        "            device, num_epochs=num_epochs_cls, scheduler=scheduler\n",
        "        )\n",
        "\n",
        "        # Plot training history\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(cls_history['train_loss'], label='Train Loss')\n",
        "        plt.plot(cls_history['val_loss'], label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(cls_history['train_acc'], label='Train Accuracy')\n",
        "        plt.plot(cls_history['val_acc'], label='Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(output_dir, 'cls_training_history.png'))\n",
        "\n",
        "    # 3. TEST ON A SAMPLE IMAGE\n",
        "\n",
        "    # Get a sample test image from validation set\n",
        "    val_imgs = [os.path.join(dataset_info['img_dir'], dataset_info['paired_files'][i])\n",
        "                for i in val_indices[:5]]  # Get 5 sample validation images\n",
        "\n",
        "    # Process each test image\n",
        "    for test_img_path in val_imgs:\n",
        "        print(f\"Processing test image: {test_img_path}\")\n",
        "\n",
        "        # Run prediction pipeline\n",
        "        results = predict_panoramic_radiograph(\n",
        "            test_img_path, seg_model, cls_model, device, seg_transform, cls_transform\n",
        "        )\n",
        "\n",
        "        # Save visualization\n",
        "        vis_path = os.path.join(output_dir, f\"result_{os.path.basename(test_img_path)}\")\n",
        "        cv2.imwrite(vis_path, cv2.cvtColor(results['result_visualization'], cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Print report\n",
        "        print(\"\\nDental Report:\")\n",
        "        for condition, count in results['report'].items():\n",
        "            print(f\"  {condition}: {count}\")\n",
        "\n",
        "    print(f\"\\nAll results saved to {output_dir}\")\n",
        "    return seg_model, cls_model\n",
        "\n",
        "\n",
        "def create_dental_report_dashboard(results, output_path):\n",
        "    \"\"\"Create an HTML dashboard for dental reports\"\"\"\n",
        "    # Create DataFrame for pie chart\n",
        "    conditions = list(results['report'].keys())\n",
        "    counts = list(results['report'].values())\n",
        "    df = pd.DataFrame({'Condition': conditions, 'Count': counts})\n",
        "\n",
        "    # Save results visualization\n",
        "    vis_path = os.path.join(os.path.dirname(output_path), 'visualization.png')\n",
        "    cv2.imwrite(vis_path, cv2.cvtColor(results['result_visualization'], cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Create HTML content\n",
        "    html_content = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Dental Radiograph Analysis Report</title>\n",
        "        <style>\n",
        "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
        "            .container {{ display: flex; flex-wrap: wrap; }}\n",
        "            .image-container {{ flex: 1; min-width: 600px; margin-right: 20px; }}\n",
        "            .report-container {{ flex: 1; min-width: 400px; }}\n",
        "            table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}\n",
        "            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "            th {{ background-color: #f2f2f2; }}\n",
        "            tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
        "            h2 {{ color: #333; }}\n",
        "            .tooth-list {{ columns: 2; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Dental Radiograph Analysis Report</h1>\n",
        "        <div class=\"container\">\n",
        "            <div class=\"image-container\">\n",
        "                <h2>Visualization</h2>\n",
        "                <img src=\"visualization.png\" alt=\"Dental Radiograph Analysis\" style=\"max-width: 100%;\">\n",
        "            </div>\n",
        "            <div class=\"report-container\">\n",
        "                <h2>Summary Report</h2>\n",
        "                <table>\n",
        "                    <tr>\n",
        "                        <th>Condition</th>\n",
        "                        <th>Count</th>\n",
        "                    </tr>\n",
        "    \"\"\"\n",
        "\n",
        "    # Add rows for each condition\n",
        "    for condition, count in results['report'].items():\n",
        "        html_content += f\"\"\"\n",
        "                    <tr>\n",
        "                        <td>{condition}</td>\n",
        "                        <td>{count}</td>\n",
        "                    </tr>\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "                </table>\n",
        "\n",
        "                <h2>Detailed Tooth Analysis</h2>\n",
        "                <div class=\"tooth-list\">\n",
        "                    <ul>\n",
        "    \"\"\"\n",
        "\n",
        "    # Add details for each tooth\n",
        "    for tooth in results['tooth_predictions']:\n",
        "        html_content += f\"\"\"\n",
        "                        <li>Tooth {tooth['tooth_id']}: {tooth['condition_name']} (Confidence: {tooth['confidence']:.2f})</li>\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "                    </ul>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    # Write HTML to file\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    print(f\"Dashboard created at {output_path}\")"
      ],
      "metadata": {
        "id": "o-PGVVs34oIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# ENTRY POINT\n",
        "#############################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup command line arguments\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Dental Radiograph Analysis Pipeline')\n",
        "    parser.add_argument('--data_dir', type=str, required=True, help='Path to dataset directory')\n",
        "    parser.add_argument('--output_dir', type=str, default='output', help='Path to output directory')\n",
        "    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for training')\n",
        "    parser.add_argument('--epochs_seg', type=int, default=30, help='Number of epochs for segmentation training')\n",
        "    parser.add_argument('--epochs_cls', type=int, default=30, help='Number of epochs for classification training')\n",
        "    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\n",
        "    parser.add_argument('--test_img', type=str, default=None, help='Path to a test image (optional)')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Run the pipeline\n",
        "    seg_model, cls_model = dental_radiograph_analysis_pipeline(\n",
        "        data_dir=args.data_dir,\n",
        "        output_dir=args.output_dir,\n",
        "        batch_size=args.batch_size,\n",
        "        num_epochs_seg=args.epochs_seg,\n",
        "        num_epochs_cls=args.epochs_cls,\n",
        "        learning_rate=args.lr\n",
        "    )\n",
        "\n",
        "    # If a test image is provided, run the prediction\n",
        "    if args.test_img:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Define transforms\n",
        "        seg_transform = transforms.Compose([\n",
        "            transforms.Resize((512, 512)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        cls_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Run prediction\n",
        "        results = predict_panoramic_radiograph(\n",
        "            args.test_img, seg_model, cls_model, device, seg_transform, cls_transform\n",
        "        )\n",
        "\n",
        "        # Save visualization\n",
        "        vis_path = os.path.join(args.output_dir, f\"result_{os.path.basename(args.test_img)}\")\n",
        "        cv2.imwrite(vis_path, cv2.cvtColor(results['result_visualization'], cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Create dashboard\n",
        "        dashboard_path = os.path.join(args.output_dir, f\"report_{os.path.basename(args.test_img).split('.')[0]}.html\")\n",
        "        create_dental_report_dashboard(results, dashboard_path)"
      ],
      "metadata": {
        "id": "j0RFgFW646Q0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}